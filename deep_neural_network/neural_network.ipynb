{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "mnist_data = fetch_openml(\"mnist_784\", parser='auto')\n",
    "x = mnist_data[\"data\"].astype(np.float32)\n",
    "y = mnist_data[\"target\"].astype(np.int32)\n",
    "\n",
    "# Normalize\n",
    "x /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "num_labels = 10\n",
    "y_one_hot = one_hot_encode(y, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "test_size = x.shape[0] - train_size\n",
    "x_train, x_test = x[:train_size].to_numpy(), x[train_size:].to_numpy()\n",
    "y_train, y_test = y_one_hot[:train_size], y_one_hot[train_size:]\n",
    "\n",
    "shuffle_index = np.random.permutation(train_size)\n",
    "x_train = x_train[shuffle_index]\n",
    "y_train = y_train[shuffle_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 784) (60000, 10)\n",
      "Test data: (10000, 784) (10000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, activation='sigmoid'):\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        else:\n",
    "            raise ValueError(\"Activation function is currently not support, please use 'relu' or 'sigmoid' instead.\")\n",
    "        \n",
    "        # Save all weights\n",
    "        self.params = self.initialize()\n",
    "        # Save all intermediate values, i.e. activations\n",
    "        self.cache = {}\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        '''\n",
    "            Derivative of ReLU is a bit more complicated since it is not differentiable at x = 0\n",
    "        \n",
    "            Forward path:\n",
    "            relu(x) = max(0, x)\n",
    "            In other word,\n",
    "            relu(x) = 0, if x < 0\n",
    "                    = x, if x >= 0\n",
    "\n",
    "            Backward path:\n",
    "            ∇relu(x) = 0, if x < 0\n",
    "                     = 1, if x >=0\n",
    "        '''\n",
    "        if derivative:\n",
    "            x = np.where(x < 0, 0, x)\n",
    "            x = np.where(x >= 0, 1, x)\n",
    "            return x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            σ(x) = 1 / 1+exp(-z)\n",
    "            \n",
    "            Backward path:\n",
    "            ∇σ(x) = exp(-z) / (1+exp(-z))^2\n",
    "        '''\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "            softmax(x) = exp(x) / ∑exp(x)\n",
    "        '''\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialize(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_layer=self.sizes[1]\n",
    "        output_layer=self.sizes[2]\n",
    "        \n",
    "        params = {\n",
    "            \"W1\": np.random.randn(hidden_layer, input_layer) * np.sqrt(1./input_layer),\n",
    "            \"b1\": np.zeros((hidden_layer, 1)) * np.sqrt(1./input_layer),\n",
    "            \"W2\": np.random.randn(output_layer, hidden_layer) * np.sqrt(1./hidden_layer),\n",
    "            \"b2\": np.zeros((output_layer, 1)) * np.sqrt(1./hidden_layer)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def initialize_momemtum_optimizer(self):\n",
    "        momemtum_opt = {\n",
    "            \"W1\": np.zeros(self.params[\"W1\"].shape),\n",
    "            \"b1\": np.zeros(self.params[\"b1\"].shape),\n",
    "            \"W2\": np.zeros(self.params[\"W2\"].shape),\n",
    "            \"b2\": np.zeros(self.params[\"b2\"].shape),\n",
    "        }\n",
    "        return momemtum_opt\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        '''\n",
    "            y = σ(wX + b)\n",
    "        '''\n",
    "        self.cache[\"X\"] = x\n",
    "        self.cache[\"Z1\"] = np.matmul(self.params[\"W1\"], self.cache[\"X\"].T) + self.params[\"b1\"]\n",
    "        self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
    "        self.cache[\"Z2\"] = np.matmul(self.params[\"W2\"], self.cache[\"A1\"]) + self.params[\"b2\"]\n",
    "        self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"])\n",
    "        return self.cache[\"A2\"]\n",
    "    \n",
    "    def back_propagate(self, y, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        current_batch_size = y.shape[0]\n",
    "        \n",
    "        dZ2 = output - y.T\n",
    "        dW2 = (1./current_batch_size) * np.matmul(dZ2, self.cache[\"A1\"].T)\n",
    "        db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(self.params[\"W2\"].T, dZ2)\n",
    "        dZ1 = dA1 * self.activation(self.cache[\"Z1\"], derivative=True)\n",
    "        dW1 = (1./current_batch_size) * np.matmul(dZ1, self.cache[\"X\"])\n",
    "        db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "        return self.grads\n",
    "    \n",
    "    def cross_entropy_loss(self, y, output):\n",
    "        '''\n",
    "            L(y, ŷ) = −∑ylog(ŷ).\n",
    "        '''\n",
    "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
    "        m = y.shape[0]\n",
    "        l = -(1./m) * l_sum\n",
    "        return l\n",
    "                \n",
    "    def optimize(self, l_rate=0.1, beta=.9):\n",
    "        '''\n",
    "            Stochatic Gradient Descent (SGD):\n",
    "            θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "            \n",
    "            Momentum:\n",
    "            v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "            θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "        '''\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for key in self.params:\n",
    "                self.params[key] = self.params[key] - l_rate * self.grads[key]\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for key in self.params:\n",
    "                self.momemtum_opt[key] = (beta * self.momemtum_opt[key] + (1. - beta) * self.grads[key])\n",
    "                self.params[key] = self.params[key] - l_rate * self.momemtum_opt[key]\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer is currently not support, please use 'sgd' or 'momentum' instead.\")\n",
    "\n",
    "    def accuracy(self, y, output):\n",
    "        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs=10, \n",
    "              batch_size=64, optimizer='momentum', l_rate=0.1, beta=.9):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer == 'momentum':\n",
    "            self.momemtum_opt = self.initialize_momemtum_optimizer()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        template = \"Epoch {}: {:.2f}s, train acc={:.2f}, train loss={:.2f}, test acc={:.2f}, test loss={:.2f}\"\n",
    "        \n",
    "        # Train\n",
    "        for i in range(self.epochs):\n",
    "            # Shuffle\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                # Batch\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "                x = x_train_shuffled[begin:end]\n",
    "                y = y_train_shuffled[begin:end]\n",
    "                \n",
    "                # Forward\n",
    "                output = self.feed_forward(x)\n",
    "                # Backprop\n",
    "                grad = self.back_propagate(y, output)\n",
    "                # Optimize\n",
    "                self.optimize(l_rate=l_rate, beta=beta)\n",
    "\n",
    "            # Evaluate performance\n",
    "            # Training data\n",
    "            output = self.feed_forward(x_train)\n",
    "            train_acc = self.accuracy(y_train, output)\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            # Test data\n",
    "            output = self.feed_forward(x_test)\n",
    "            test_acc = self.accuracy(y_test, output)\n",
    "            test_loss = self.cross_entropy_loss(y_test, output)\n",
    "            print(template.format(i+1, time.time()-start_time, train_acc, train_loss, test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2.42s, train acc=0.96, train loss=0.14, test acc=0.95, test loss=0.15\n",
      "Epoch 2: 3.67s, train acc=0.97, train loss=0.10, test acc=0.96, test loss=0.12\n",
      "Epoch 3: 5.16s, train acc=0.98, train loss=0.08, test acc=0.97, test loss=0.10\n",
      "Epoch 4: 6.52s, train acc=0.98, train loss=0.07, test acc=0.97, test loss=0.10\n",
      "Epoch 5: 8.35s, train acc=0.98, train loss=0.06, test acc=0.97, test loss=0.10\n",
      "Epoch 6: 9.70s, train acc=0.99, train loss=0.04, test acc=0.97, test loss=0.08\n",
      "Epoch 7: 11.19s, train acc=0.99, train loss=0.04, test acc=0.97, test loss=0.09\n",
      "Epoch 8: 12.40s, train acc=0.99, train loss=0.04, test acc=0.97, test loss=0.09\n",
      "Epoch 9: 13.93s, train acc=0.99, train loss=0.03, test acc=0.97, test loss=0.09\n",
      "Epoch 10: 15.40s, train acc=0.99, train loss=0.02, test acc=0.98, test loss=0.08\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid + Momentum\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='sigmoid')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='momentum', l_rate=4, beta=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2.91s, train acc=0.89, train loss=0.40, test acc=0.89, test loss=0.39\n",
      "Epoch 2: 3.98s, train acc=0.90, train loss=0.34, test acc=0.91, test loss=0.32\n",
      "Epoch 3: 5.11s, train acc=0.91, train loss=0.31, test acc=0.92, test loss=0.30\n",
      "Epoch 4: 6.69s, train acc=0.92, train loss=0.29, test acc=0.92, test loss=0.28\n",
      "Epoch 5: 8.07s, train acc=0.92, train loss=0.28, test acc=0.92, test loss=0.27\n",
      "Epoch 6: 9.19s, train acc=0.92, train loss=0.27, test acc=0.92, test loss=0.27\n",
      "Epoch 7: 10.33s, train acc=0.92, train loss=0.27, test acc=0.92, test loss=0.26\n",
      "Epoch 8: 11.57s, train acc=0.92, train loss=0.26, test acc=0.93, test loss=0.26\n",
      "Epoch 9: 12.60s, train acc=0.93, train loss=0.25, test acc=0.93, test loss=0.25\n",
      "Epoch 10: 13.46s, train acc=0.93, train loss=0.25, test acc=0.93, test loss=0.25\n"
     ]
    }
   ],
   "source": [
    "# ReLU + SGD\n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 10], activation='relu')\n",
    "dnn.train(x_train, y_train, x_test, y_test, batch_size=128, optimizer='sgd', l_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist_data = fetch_openml(\"mnist_784\", parser='auto')\n",
    "X, y = mnist_data[\"data\"].astype(np.int64), mnist_data[\"target\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to integers\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# Normalize the input data\n",
    "X = X / 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:12:48.377988: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400/1400 [==============================] - 3s 2ms/step - loss: 0.3186 - accuracy: 0.9065 - val_loss: 0.1440 - val_accuracy: 0.9564\n",
      "Epoch 2/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.1469 - accuracy: 0.9555 - val_loss: 0.1186 - val_accuracy: 0.9641\n",
      "Epoch 3/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.1117 - accuracy: 0.9662 - val_loss: 0.1100 - val_accuracy: 0.9643\n",
      "Epoch 4/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0916 - accuracy: 0.9709 - val_loss: 0.0984 - val_accuracy: 0.9694\n",
      "Epoch 5/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0781 - accuracy: 0.9752 - val_loss: 0.0854 - val_accuracy: 0.9740\n",
      "Epoch 6/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0687 - accuracy: 0.9783 - val_loss: 0.0898 - val_accuracy: 0.9738\n",
      "Epoch 7/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0609 - accuracy: 0.9805 - val_loss: 0.0827 - val_accuracy: 0.9752\n",
      "Epoch 8/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0541 - accuracy: 0.9823 - val_loss: 0.0877 - val_accuracy: 0.9735\n",
      "Epoch 9/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0538 - accuracy: 0.9828 - val_loss: 0.0946 - val_accuracy: 0.9723\n",
      "Epoch 10/10\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.0478 - accuracy: 0.9835 - val_loss: 0.0843 - val_accuracy: 0.9749\n",
      "438/438 - 0s - loss: 0.0966 - accuracy: 0.9736 - 247ms/epoch - 563us/step\n",
      "Test accuracy: 0.973642885684967\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
